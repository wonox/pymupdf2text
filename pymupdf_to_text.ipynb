{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz # import PyMuPDF\n",
    "from pathlib import Path\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# テキストを置換：項目名を△△で囲む\n",
    "def text_replace(text):\n",
    "\n",
    "   # text = re.sub(r'\\n([^\\d\\.|^[A-Z])', r'\\1', text)  # 不要な改行の削除\n",
    "   text = re.sub(r'\\n([\\d\\.]{2,}[A-Z]*)', r'\\n△\\1△', text)\n",
    "   text = re.sub(r'^([\\d\\.]{2,})', r'\\n△\\1△', text) # △0.1△ \n",
    "   text = re.sub(r'([^A-Z][A-Z]\\d[\\.\\d]*)', r'\\n△\\1△', text) # △B2.2△  \n",
    "   text = re.sub(r'→ \\n△(.+)△', r'→\\1', text) # (→ \\n△\n",
    "   text = re.sub(r' \\n△(.+)△ (から)\\n△(.+)△', r'\\1\\2\\3', text)\n",
    "   text = re.sub(r'^(第.+?章)', r'\\1△', text)\n",
    "   text = re.sub(r'△(\\d\\d\\d)△', r'\\1', text)\n",
    "   return text\n",
    "\n",
    "# pymupdfでPDFをテキストに変換\n",
    "def pdf2text(filepath):\n",
    "   basename_without_ext = os.path.splitext(os.path.basename(filepath))[0]\n",
    "   print(basename_without_ext)\n",
    "\n",
    "   doc = fitz.open(filepath)  # open a supported document\n",
    "\n",
    "   text = \"\"\n",
    "   # text = chr(12).join([page.get_text() for page in doc])\n",
    "   text = \"\".join([page.get_text() for page in doc])\n",
    "   text = text_replace(text)\n",
    "   return text, basename_without_ext\n",
    "\n",
    "def text2df(text):\n",
    "   # split()で文字列を分割して取得したリストにstrip()を適用する。\n",
    "   # 空白を含むカンマ区切り文字列の余分な空白を除去してリスト内包表記でリスト化\n",
    "   # list_text = [x.strip() for x in text[0].split('△')]\n",
    "   list_text = [x.strip() for x in text.split('△')]\n",
    "   list_text = [a for a in list_text if a != '']  # 内包表記で空の要素を駆逐する\n",
    "   # print(list_text)\n",
    "\n",
    "   # キーとバリューが交互に並ぶリストを辞書にする\n",
    "   dict_text = dict(zip(list_text[0::2], list_text[1::2]))\n",
    "   #print(dict_text)\n",
    "\n",
    "   # 異なる長さのリストを含む辞書をpd.DataFrameに変換\n",
    "   df2 = pd.DataFrame.from_dict(dict_text, orient='index')\n",
    "   # df2\n",
    "   df2.to_excel('pndas_to_excel.xlsx', sheet_name='new_sheet_name')\n",
    "   return df2\n",
    "\n",
    "\n",
    "filepath = r\"pdf/CM14.pdf\"\n",
    "text = pdf2text(filepath)\n",
    "# 戻り値の型はタプルとなる。text, basename_without_ext\n",
    "dataframe = text2df(text[0])\n",
    "\n",
    "# path_w = r\"cm0.txt\"\n",
    "other_filepath = os.path.join(os.path.dirname(filepath), text[1]) # basename_without_ext)\n",
    "other_ext_filepath = other_filepath + '.txt'\n",
    "\n",
    "with open(other_ext_filepath, mode='w') as f:\n",
    "    f.write(text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "files = glob.glob(\"pdf/*.pdf\")\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 旧（現行）CM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Pythonのスクレイピングで文字だけを抜き出す汎用的な方法 #Python3 - Qiita\\nsearchsearchSearchLoginSignupsearchTrendQiita Engineer Festa 2024QuestionOfficial EventOfficial ColumnsignpostCareerOrganization\\n138Go to list of users who liked144more_horizinfoMore than 1 year has passed since last update.@poorkoPythonのスクレイピングで文字だけを抜き出す汎用的な方法PythonscrapingPython3BeautifulSoupGoogleColaboratoryLast updated at 2022-03-21Posted at 2018-12-16\\nPythonのスクレイピングで文字だけきれいに抽出する汎用的な方法\\n初投稿です。\\npythonのスクレイピングで複数のサイトから文字だけ全部抜き出そうとする際に、うまく機能する汎用的なやり方を理解するのにわりと時間がかかったため、ここにしたためておきます。\\nまず、英語でどんぴしゃり回答していらっしゃる方がいたので、英語いける方はこちらのURLを見ていただければ試合終了です。\\nExtracting text from HTML file using Python\\n私はコードを調べつつでないと理解できないPrograming初心者で、割と処理の理解に時間がかかったので、やり方を０から簡単に解説します。必要なところだけさくっと見てください。\\n事前準備\\nインストール\\nimport requests\\nimport pandas as pd\\nfrom bs4 import BeautifulSoup\\nHTML取得\\nhtml=requests.get(\"https://www.updateurself.com/2018/11/14/the-way-to-challenge/\").text\\nsoup=BeautifulSoup(html,\"html.parser\")\\n#print(soup.prettify)\\nちなみにこのURLは私がやっているプログラミングとは何の関係もないブログです。\\n文字を取り出していく\\n###1, scriptやstyleを含む要素を削除する\\nfor script in soup([\"script\", \"style\"]):\\nscript.decompose()\\n#print(soup)\\n.decompose()は、削除のメソッドです。\\n###2, テキストのみを取得=タグは全部取る\\ntext=soup.get_text()\\n#print(text)\\n引用：Extracting text from HTML file using Python\\n.get_text()は、テキストのみを取得する、つまりタグは取らないメソッドです。\\n###3, textを改行ごとにリストに入れて、リスト内の要素の前後の空白を削除\\nlines= [line.strip() for line in text.splitlines()]\\n引用：Extracting text from HTML file using Python\\n【補足】ここはリスト内包表記です。書き下すと以下のことをやっています。\\nlines=[]\\nfor line in text.splitlines():\\nlines.append(line.strip())\\n#print(lines)\\n.strip()では引数に指定したものを文字列の文頭文末から削除することができます。\\n今回のように引数に何も指定しなければ空白（改行等中身のないもの）を全削除です。\\n.splitlines()は改行ごとに要素をリストに入れます。\\n出力はこんな感じになります。（最初の部分のみ）\\n[\\'\\', \\'\\', \\'\\', \\'\\', \\'\\', \\'\\', \\'\\', \\'\\', \\'\\', \\'\\', \\'\\', \\'\\', \\'\\', \\'\\', \\'人生に目標が無いから何？目標より大事なアレを見つける３つの方法 - Update yourself\\', \\'\\', \\'\\', \\'\\', \\'\\', \\'\\', \\'\\', \\'\\', \\'\\', \\'\\', \\'\\', \\'\\', \\'\\', \\'\\', \\'\\', \\'\\', \\'\\', \\'\\', \\'\\', \\'\\', \\'\\', \\'\\', \\'\\', \\'\\', \\'\\', \\'\\', \\'\\', \\'\\', \\'\\', \\'\\', \\'\\', \\'\\', \\'\\', \\'\\', \\'\\', \\'\\', \\'\\', \\'\\', \\'\\', \\'\\', \\'\\', \\'\\', \\'\\', \\'\\', \\'\\', \\'\\', \\'\\', \\'\\', \\'\\', \\'\\', \\'\\', \\'\\', \\'\\', \\'\\', \\'\\', \\'\\', \\'\\', \\'\\', \\'\\', \\'\\', \\'\\', \\'\\', \\'\\', \\'\\', \\'\\', \\'\\', \\'\\', \\'\\', \\'\\', \\'\\', \\'\\', \\'\\', \\'\\', \\'\\', \\'\\', \\'\\', \\'\\', \\'\\', \\'\\', \\'\\', \\'\\', \\'\\', \\'\\', \\'\\', \\'\\', \\'\\', \\'\\', \\'\\', \\'\\', \\'\\', \\'\\', \\'\\', \\'\\', \\'\\', \\'ホーム\\', \\'悩み\\', \\'\\', \\'こころ\\', \\'人間関係\\', \\'恋愛\\', \\'\\', \\'\\', \\'思考法\\',\\n(冒頭部分改行続きのため中身に何も入っていないものが多いですね)\\n4, リストの空白要素以外をすべて文字列に戻す\\ntext=\"\\\\n\".join(line for line in lines if line)\\n.joinはリストを文字列に変換します。\\nここで、それぞれのリストの中身ごとに改行して文字列に戻していきます。このとき注意点は、「\\'\\'」のような中身が何もないlineは無視することです。そのためにif lineが必要です。\\n以上で完成です。最終アウトプットはこんな感じです。\\n参考URL\\nhttps://stackoverflow.com/questions/328356/extracting-text-from-html-file-using-python\\nhttps://python-reference.readthedocs.io/en/latest/index.html\\n138Go to list of users who liked144comment2Go to list of commentsRegister as a new user and use Qiita more convenientlyYou get articles that match your needsYou can efficiently read back useful informationYou can use dark themeWhat you can do with signing upSign upLogin138Go to list of users who liked144more_horiz\\nHow developers code is here.© 2011-2024Qiita Inc.Guide & HelpAboutTermsPrivacyGuidelineDesign GuidelineFeedbackHelpAdvertisementContentsRelease NoteOfficial EventOfficial ColumnAdvent CalendarQiita AwardAPICareerSNS@Qiita@qiita_milestone@qiitapoi@QiitaOur serviceQiita TeamQiita ZineOfficial ShopCompanyAbout UsCareersQiita Blog'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# https://qiita.com/poorko/items/9140c75415d748633a10\n",
    "# Pythonのスクレイピングで文字だけを抜き出す汎用的な方法\n",
    "import re\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "# urlを指定してBeautifulSoupオブジェクトを返す\n",
    "def scrape_html(url):\n",
    "  html = requests.get( url ) # .text\n",
    "  soup = BeautifulSoup(html.content.decode(\"utf-8\", \"ignore\"), \"html.parser\")\n",
    "  # print(soup.prettify)\n",
    "  return soup\n",
    "\n",
    "# BeautifulSoupオブジェクトを引数に、テキストを返す\n",
    "def get_text(soup):\n",
    "  # scriptやstyleを含む要素を削除(decompose)する\n",
    "  for script in soup([\"script\",  \"style\"]):\n",
    "      script.decompose()\n",
    "  # print(soup)\n",
    "  # タグを削除してテキストを取得\n",
    "  text = soup.get_text()\n",
    "  # print(text)\n",
    "  # textを改行ごとにリストに入れて、リスト内の要素の前後の空白を削除(strip)、内包表記でリスト化\n",
    "  lines = [line.strip() for line in text.splitlines()]\n",
    "  #lines = []\n",
    "  # for line in text.splitlines():\n",
    "  #  lines.append(line.strip())\n",
    "  # print(lines)\n",
    "\n",
    "  text = \"\\n\".join(line for line in lines if line)\n",
    "  # print(text)\n",
    "  return text\n",
    "\n",
    "# URLを引数に、リンクのリストを出力\n",
    "def list_links(url):\n",
    "    parse_html = scrape_html(url)\n",
    "    title_lists = parse_html.find_all(\"a\")\n",
    "\n",
    "    # hrefを取得してリスト化\n",
    "    name_list = []\n",
    "    url_list = []\n",
    "    for i in title_lists:\n",
    "        name_list.append(i.string)\n",
    "        url_list.append(i.get('href'))   # (i.attrs[\"href\"]) getはなければNoneが返ってきます\n",
    "\n",
    "    # リストの空の要素を消す\n",
    "    # [x for x in url_list if x != '']\n",
    "    url_list_b = [x for x in url_list if x is not None]\n",
    "    # 先頭が数字のリンクを抽出し、リスト化\n",
    "    url_list_b = [x for x in url_list_b if re.match('^\\d', x)]\n",
    "    # print(url_list_b)\n",
    "    \n",
    "    return url_list_b\n",
    "\n",
    "# URLを引数にurlをパースして、リンクのリストを返す\n",
    "def url_parse(access_url):\n",
    "   parsed_url = urlparse(access_url) \n",
    "   path_list = parsed_url.path.split(\"/\")[-2:]\n",
    "   # フォーマットする \n",
    "   # url = '{uri.scheme}://{uri.netloc}/path_list[0]/'.format(uri=urlparse(access_url))\n",
    "   url_d = parsed_url.scheme + '://' + parsed_url.netloc + '/' + path_list[0] + '/'\n",
    "   print(url_d)\n",
    "\n",
    "   # リンク先をリスト化\n",
    "   url_list_b = list_links(access_url)\n",
    "   # url_full = url_d + url_list_b[0]\n",
    "   # htmlファイルをスクレイプする\n",
    "   # print(scrape_html(url_d + url_list_b[0])) #.decode(\"utf-8\")\n",
    "   # html_text = get_text(scrape_html(url_full))\n",
    "\n",
    "   # urlのリストから、url_fullをリスト化\n",
    "   url_full = [url_d + i for i in url_list_b]\n",
    "   \"\"\"\n",
    "   url_full = []\n",
    "   for i in url_list_b:\n",
    "      url_full.append(url_d + i)\n",
    "      # html_text += get_text(scrape_html(url_full))\n",
    "   \"\"\"\n",
    "   return url_full\n",
    "\n",
    "# テスト用\n",
    "get_text(scrape_html(\"https://qiita.com/poorko/items/9140c75415d748633a10\"))\n",
    "# list_links(\"https://catill.bitbucket.io/CM/mokuji.html\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://catill.bitbucket.io/CM/\n"
     ]
    }
   ],
   "source": [
    "access_url = \"https://catill.bitbucket.io/CM/mokuji.html\"\n",
    "url_full = url_parse(access_url)\n",
    "# print(url_full)\n",
    "for i in url_full:\n",
    "    html_text += get_text(scrape_html(i))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\onowa\\pumupdf\n",
      "c:\\Users\\onowa\\html_text\n",
      "c:\\Users\\onowa\\html_text.txt\n"
     ]
    }
   ],
   "source": [
    "# print(html_text)\n",
    "import os\n",
    "\n",
    "path = os.getcwd()\n",
    "print(path)\n",
    "other_filepath = os.path.join(os.path.dirname(path), 'html_text') # basename_without_ext)\n",
    "print(other_filepath)\n",
    "other_ext_filepath = other_filepath + '.txt'\n",
    "print(other_ext_filepath)\n",
    "\n",
    "\n",
    "with open(other_ext_filepath, mode='w') as f:\n",
    "    f.write(html_text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
